{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g8P_laMcSQNk"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCjHDFqa7ttO"
      },
      "source": [
        "#Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d3xVez-WTeww"
      },
      "outputs": [],
      "source": [
        "def q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        for step in range(num_steps_per_episode):\n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state, :])\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * np.max(q_table[next_state,:]))\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    print(f'Episode {episode} finished')\n",
        "    return q_table, rewards_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yGopsD0IWpDO"
      },
      "outputs": [],
      "source": [
        "def play(env, q_table, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state, :])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2l8BKi9TSqRe"
      },
      "outputs": [],
      "source": [
        "def play_multiple_times(env, q_table, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, q_table)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPJ-yTr2Jin"
      },
      "source": [
        "## FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VMx8r6bo2ZHP"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs-EbCUUSvf2",
        "outputId": "37adaf95-7cbe-4e95-8ca4-796c53e220ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 741/1000\n",
            "Average number of steps: 37.178137651821864\n",
            "Running time:  19.070709943771362\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "end = time.time()\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print(\"Running time: \", end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L83LspWT2NvA"
      },
      "source": [
        "## FrozenLake8x8-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wJe6ZWne2T5c"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.9\n",
        "learning_rate = 0.8\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.001\n",
        "epsilon_decay_rate = 0.00005\n",
        "\n",
        "num_episodes = 250000\n",
        "num_steps_per_episode = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cydm1iuN5tEY",
        "outputId": "702bfd02-adbd-4dda-ff1f-611e883bd653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 249999 finished\n",
            "Number of successes: 499/1000\n",
            "Average number of steps: 91.7434869739479\n",
            "Running time:  509.27175998687744\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "end = time.time()\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print(\"Running time: \", end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kSOtjIO565-"
      },
      "source": [
        "## Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHmxi_IS5_jj",
        "outputId": "325a2c72-697e-47fa-e84e-98c66d9c4068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100\n",
        "\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3zAjSdH58YR",
        "outputId": "d8763311-6b2b-4323-9d09-14fcbbe771ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.058\n",
            "Running time:  9.554579496383667\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "q_table, rewards_all = q_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "end = time.time()\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print(\"Running time: \", end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deNzWnqS7Ved"
      },
      "source": [
        "#SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bm4CcsAzSx-f"
      },
      "outputs": [],
      "source": [
        "def SARSA_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    rewards_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "\n",
        "        exploration = random.uniform(0,1)\n",
        "        if exploration < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])\n",
        "\n",
        "        for step in range(num_steps_per_episode):\n",
        "            \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            exploration = random.uniform(0,1)\n",
        "            if exploration < epsilon:\n",
        "                next_action = env.action_space.sample()\n",
        "            else:\n",
        "                next_action = np.argmax(q_table[next_state, :])\n",
        "\n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * q_table[next_state,next_action])\n",
        "\n",
        "\n",
        "            reward_episode += reward\n",
        "            action = next_action\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    print(f'Episode {episode} finished')\n",
        "    return q_table, rewards_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umMrWFH96TP0"
      },
      "source": [
        "## FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NBogAUEK6ahT"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adBm178P61ys",
        "outputId": "9dff87a3-1540-4e2e-bbe5-2f5a50ac0fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 629/1000\n",
            "Average number of steps: 37.20826709062003\n",
            "Running time:  12.387576818466187\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "q_table, rewards_all = SARSA_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "end = time.time()\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print(\"Running time: \", end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qGeu5P96Vjf"
      },
      "source": [
        "## FrozenLake8x8-v0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aUdakxS-7H1y"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.9\n",
        "learning_rate = 0.8\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.001\n",
        "epsilon_decay_rate = 0.00005\n",
        "\n",
        "num_episodes = 250000\n",
        "num_steps_per_episode = 400\n",
        "\n",
        "# q_table, rewards_all = SARSA_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dx-ZRH1s7Kyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14660aa5-bd98-4712-92c7-450d00703619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 249999 finished\n",
            "Number of successes: 729/1000\n",
            "Average number of steps: 86.58710562414267\n",
            "Running time:  313.45714259147644\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "q_table, rewards_all = SARSA_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "end = time.time()\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print(\"Running time: \", end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IatwmnFk6X_N"
      },
      "source": [
        "## Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s8_Ik0MB7Rsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e1cbfe-b356-4807-dd9e-1db95eb08a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# Initialize Q-value table randomly\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_episodes = 20000\n",
        "num_steps_per_episode = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f4wglDqf8IRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aee19ff-c202-49f2-b2de-08bb5d33328a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19999 finished\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.094\n",
            "Running time:  5.513466835021973\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "q_table, rewards_all = SARSA_learning(env, num_episodes, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "end = time.time()\n",
        "play_multiple_times(env, q_table, 1000)\n",
        "print(\"Running time: \", end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQaVM0zpHLq8"
      },
      "source": [
        "# Nhận xét"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FrozenLake-v0 | Q-learning  | SARSA \n",
        "-------------------|-------------------|------------------\n",
        "Running time |19.070709943771362 |12.387576818466187\n",
        "Number of successes       | 741/1000 |629/1000\n",
        "Average number of steps       | 37.178137651821864| 37.20826709062003\n"
      ],
      "metadata": {
        "id": "dFDFV-8MpOwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FrozenLake8x8-v0 | Q-learning  | SARSA\n",
        "-------------------|-------------------|------------------\n",
        "Running time |509.27175998687744 | 313.45714259147644\n",
        "Number of successes       | 499/1000 |729/1000\n",
        "Average number of steps       |91.7434869739479 | 86.58710562414267\n"
      ],
      "metadata": {
        "id": "EFaGS_0ZrldH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taxi-v3 | Q-learning  | SARSA\n",
        "-------------------|-------------------|------------------\n",
        "Running time |9.554579496383667 |5.513466835021973\n",
        "Number of successes       | 1000/1000 |1000/1000\n",
        "Average number of steps       | 13.058 | 13.094\n"
      ],
      "metadata": {
        "id": "uxS33Bv2r3cN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Thời gian tìm lời giải của 2 thuật toán có sự chênh lệch lớn, trong cả 3 trường hợp SARSA learing có thời gian tìm lời giải chỉ xấp xỉ 3/5 Q learning\n",
        "*  Số bước giải trung bình ở 2 thuật toán không quá chênh lệch\n",
        "* Q learning có tỉ lệ thắng cao hơn ở map FrozenLake-v0 , còn SARSA có tỉ lệ thắng cao hơn ở map FronzenLake8x8-v0"
      ],
      "metadata": {
        "id": "oq5gzC0rybtY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "20520781_Q_SARSA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}